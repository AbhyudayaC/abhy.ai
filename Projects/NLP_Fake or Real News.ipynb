{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IDbPTnTqlyp-"
   },
   "source": [
    "\n",
    "\n",
    "# Fake News Detection\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The purpose of this notebook is to detect if the news is fake or real\n",
    "\n",
    "\n",
    "**Who worked on this assignment:**\n",
    "\n",
    "- Abhyudaya Choumal\n",
    "- Amal AlMutawa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6eR3OBFlyqA"
   },
   "source": [
    "# Importing Libraries  \n",
    "\n",
    "For the algorithms to work, a number of libraries will need to be imported in order to work on this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86J8esNnnZlg"
   },
   "outputs": [],
   "source": [
    "#Basic libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "import errno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Parsing\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "from nltk.draw import tree\n",
    "\n",
    "#Tagging \n",
    "\n",
    "#NER\n",
    "\n",
    "\n",
    "#Semantics\n",
    "\n",
    "\n",
    "#Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj-y9Q5WoZUG"
   },
   "source": [
    "# Loading the datasets \n",
    "\n",
    "Below we upload both the train and the test datasets provided in the classroom\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGHRt5RSynoC"
   },
   "source": [
    "***Dataset Details:***\n",
    "\n",
    "*fake_or_real_news_training:*\n",
    "- **ID:** ID of the tweet\n",
    "\n",
    "- **Title:** Title of the news report\n",
    "- **Text:** Textual content of the news report \n",
    "- **Label:** Target Variable (FAKE, REAL)\n",
    "- **X1, X2** additional fields *\n",
    "\n",
    "*fake_or_real_news_test:*\n",
    "- ID, title and text\n",
    "- Predict Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEP10rQaoeyg"
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "fake_or_real_news_training = pd.read_csv(\"fake_or_real_news_training.csv\") \n",
    "\n",
    "#Test data\n",
    "fake_or_real_news_test  = pd.read_csv(\"fake_or_real_news_test.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ikhgPlN_lyqC"
   },
   "source": [
    "#Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRa7hmjDthAP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhyudaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'newsTokenizer' from 'nltk.tokenize' (/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f2570a2c3f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnewsTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'´΄’…“”–—―»«'\u001b[0m \u001b[0;31m# string.punctuation misses these.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'newsTokenizer' from 'nltk.tokenize' (/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py)"
     ]
    }
   ],
   "source": [
    "#this code is copied from: https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/6-1-Twitter-Stream-and-Cleaning-newss.ipynb\n",
    "#it needs to be cleaned/adjusted\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import newsTokenizer\n",
    "\n",
    "punctuation += '´΄’…“”–—―»«' # string.punctuation misses these.\n",
    "\n",
    "cache_english_stopwords = stopwords.words('english') # Could speed up code by making this a set\n",
    "\n",
    "def news_clean(news):\n",
    "    print('Original news:', news, '\\n')\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    news_no_special_entities = re.sub(r'\\&\\w*;', '', news)\n",
    "    print('No special entitites:', news_no_special_entities, '\\n')\n",
    "    # Remove tickers (Clickable stock market symbols that work like hashtags and start with dollar signs instead)\n",
    "    news_no_tickers = re.sub(r'\\$\\w*', '', news_no_special_entities) # Substitute. $ needs to be escaped because it means something in regex. \\w means alphanumeric char or underscore.\n",
    "    print('No tickers:', news_no_tickers, '\\n')\n",
    "    # Remove hyperlinks\n",
    "    news_no_hyperlinks = re.sub(r'https?:\\/\\/.*\\/\\w*', '', news_no_tickers)\n",
    "    print('No hyperlinks:', news_no_hyperlinks, '\\n')\n",
    "    # Remove hashtags\n",
    "    news_no_hashtags = re.sub(r'#\\w*', '', news_no_hyperlinks)\n",
    "    print('No hashtags:', news_no_hashtags, '\\n')\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    news_no_punctuation = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', news_no_hashtags)\n",
    "    print('No punctuation:', news_no_punctuation, '\\n')\n",
    "    # Remove words with 2 or fewer letters (Also takes care of RT)\n",
    "    news_no_small_words = re.sub(r'\\b\\w{1,2}\\b', '', news_no_punctuation) # \\b represents a word boundary\n",
    "    print('No small words:', news_no_small_words, '\\n')\n",
    "    # Remove whitespace (including new line characters)\n",
    "    news_no_whitespace = re.sub(r'\\s\\s+', ' ', news_no_small_words)\n",
    "    news_no_whitespace = news_no_whitespace.lstrip(' ') # Remove single space left on the left\n",
    "    print('No whitespace:', news_no_whitespace, '\\n')\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    news_no_emojis = ''.join(c for c in news_no_whitespace if c <= '\\uFFFF') # Apart from emojis (plane 1), this also removes historic scripts and mathematical alphanumerics (also plane 1), ideographs (plane 2) and more.\n",
    "    print('No emojis:', news_no_emojis, '\\n')\n",
    "    # Tokenize: Change to lowercase, reduce length and remove handles\n",
    "    tknzr = newsTokenizer(preserve_case=False, reduce_len=True, strip_handles=True) # reduce_len changes, for example, waaaaaayyyy to waaayyy.\n",
    "    tw_list = tknzr.tokenize(news_no_emojis)\n",
    "    print('news tokenize:', tw_list, '\\n')\n",
    "    # Remove stopwords\n",
    "    list_no_stopwords = [i for i in tw_list if i not in cache_english_stopwords]\n",
    "    print('No stop words:', list_no_stopwords, '\\n')\n",
    "    # Final filtered news\n",
    "    news_filtered =' '.join(list_no_stopwords) # ''.join() would join without spaces between words.\n",
    "    print('Final news:', news_filtered)\n",
    "\n",
    "s = '    RT @Amila #Test\\nTom\\'s newly listed Co. &amp; Mary\\'s unlisted     Group to supply tech for nlTK.\\nh.. $TSLA $AAPL https:// t.co/x34afsfQsh'\n",
    "news_clean(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CynLNeplyqI"
   },
   "source": [
    "# Training & Solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvkMoTnFlyqI"
   },
   "source": [
    "***TO DELETE***\n",
    "\n",
    "The professor wants us to use:\n",
    "\n",
    "from slide:\n",
    "\n",
    "**Classification problem:**\n",
    "- News Report (document)→Class: (FAKE, REAL)\n",
    "\n",
    "**Try text-related classifiers**\n",
    "- Naive Bayes \n",
    "- MaxEnt \n",
    "- SVM\n",
    "\n",
    "**NLTK+SKLearn provides you anything you need**\n",
    "- NLP Pre-processing \n",
    "- Classifiers \n",
    "- CV-evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7ZW830t2yUU"
   },
   "source": [
    "**1. Naive Bayes**\n",
    "We first tried the approach of Naive Bayes to see the results we get\n",
    "\n",
    "link to code (i googled): https://dzone.com/articles/naive-bayes-tutorial-naive-bayes-classifier-in-pyt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJhwwpAQ28KA"
   },
   "outputs": [],
   "source": [
    "# code for Naive Bayes (also we need an output file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywP9_Gg527kS"
   },
   "source": [
    "*Conclusion for Naive Bayes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCstwCeu4g2s"
   },
   "source": [
    "**2. MaxEnt**\n",
    "\n",
    "\n",
    "link to code (i googled): https://github.com/PythonCharmers/maxentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P67iSuX4g2u"
   },
   "outputs": [],
   "source": [
    "# code for MaxEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP7Hth3F4g2y"
   },
   "source": [
    "*Conclusion for MaxEnt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYNP-TlO4hlQ"
   },
   "source": [
    "**3. SVM**\n",
    "\n",
    "\n",
    "link to code (i googled): https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlkSbt-V4hlT"
   },
   "outputs": [],
   "source": [
    "# code for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-GCVJF34hlb"
   },
   "source": [
    "*Conclusion for SVM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6uOr1cq4leU"
   },
   "source": [
    "**NLP Pre-processing**\n",
    "He asked to use the ones we did in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQcIT8564leW"
   },
   "outputs": [],
   "source": [
    "# code for Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7OR6acvM4leb"
   },
   "source": [
    "*feedback on pre-processing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMe1-NIY4mke"
   },
   "source": [
    "**Classifiers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwqaIrqK4mkh"
   },
   "outputs": [],
   "source": [
    "# code for Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbJxaJmE4mkk"
   },
   "source": [
    "*Comments?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XhgcgnYl4nNu"
   },
   "source": [
    "**CV-evalution**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XWmNNWvT4nN4"
   },
   "outputs": [],
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ocwh4ar24nN6"
   },
   "source": [
    "\n",
    "*Comments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mODcxT0B4flD"
   },
   "source": [
    "**TF_IDF**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYx3gWbh4flF"
   },
   "outputs": [],
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_QNEU5x4flK"
   },
   "source": [
    "*Conclusion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_rkA79j6HMC"
   },
   "source": [
    "**N-grams**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKtwez5z6HMG"
   },
   "outputs": [],
   "source": [
    "# code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFQ24jgl6HMK"
   },
   "source": [
    "*Comments*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0PVM1NQ6TNR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rQDN4WC6fbK"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "From the differnt methods we tried, the XXX was the best method giving us xx% accuracy. It...."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "j6eR3OBFlyqA",
    "Lj-y9Q5WoZUG",
    "ikhgPlN_lyqC",
    "5CynLNeplyqI"
   ],
   "name": "NLP_Abhy-Amal_Fake or Real.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
